{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "### Assignment 6\n",
    "After training a skip-gram model in 5_word2vec.ipynb, the goal of this notebook is to train a LSTM character model over Text8 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the data into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "data_path = '/Users/heany/code/AI-For-NLP/dataset'\n",
    "filename = os.path.join(data_path,'text8.zip')\n",
    "\n",
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        data = tf.compat.as_str(f.read(f.namelist()[0]))\n",
    "    return data\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size {}'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000 thoritarian political structures and coercive economic instituti\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[-64:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility functions to map characters to vlcabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print(\"Unexpected character: {}\".format(char))\n",
    "        return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a ter'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n11 个batch，每个batch 64个letter， \\n每个batch取一个letter组成一个string，\\n最后形、成 64个string，每个string 11个letter\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"\n",
    "        Generate the next array of batches from the data. \n",
    "        The array consists of the last batch of the previous array, \n",
    "        followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    # print('S lenght : {}'.format(len(s)))\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)  # shape=(11,64,27)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))  # shape = (64,11) shown in the below. \n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "\n",
    "\"\"\"\n",
    "11 个batch，每个batch 64个letter， \n",
    "每个batch取一个letter组成一个string，\n",
    "最后形、成 64个string，每个string 11个letter\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 64, 27)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_batches.next()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.next()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Variables:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    \n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])), \n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output), \n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298156 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "czhdnu xxnalce ktr cvpeekgaas htfirca w ua  n fe f zonjg     h  neaimyaayuw etrr\n",
      "iorxobiqcelevxdz b rtz  hhlmuzhaaipll h wdlufna hmrxts qdh n ky   ddsdolt rquuco\n",
      "mc hewrirgynhcvtow dbeddcsom dakl enjjonziqyppz kaae edttguf mhbie ru   tz t  vg\n",
      "zkleeio uvzitdxia riaf  ffdvkfrm mrcwdkniynkf  nkxit lvrdtr  csubqzr ijbqh zk ns\n",
      "rjejo echkyftayhyrddrxs erak  d fijkxoyghbebftf zttovasm teuzjccwnnret teutkstvv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 100: 2.590577 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.67\n",
      "Validation set perplexity: 10.18\n",
      "Average loss at step 200: 2.240012 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 8.53\n",
      "Average loss at step 300: 2.094111 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 8.06\n",
      "Average loss at step 400: 1.997379 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 500: 1.936142 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.63\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.905819 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 700: 1.857976 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 800: 1.814702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 900: 1.824425 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.90\n",
      "Validation set perplexity: 6.26\n",
      "Average loss at step 1000: 1.819598 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "================================================================================\n",
      "x in the a ruess infipited cendlount in eorugh und and pererved willite of the l\n",
      "buy nially as spyticly uppliedity cimely lengratury is fer empil hunrations hilk\n",
      "em proviration igualy enceiveric be fadefle lave conssided and unric wixhey ence\n",
      "jer a ro dreqing the formeg in the rbttiont rumoble of roducy k ghek kormens ane\n",
      "hrabted couller one zero mehwre forpeors wadi alcessef to kill remicted two feve\n",
      "================================================================================\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1100: 1.772934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1200: 1.745091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1300: 1.734160 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1400: 1.743572 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1500: 1.731522 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1600: 1.741326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1700: 1.710507 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1800: 1.670964 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.643294 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.691362 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "piring conital ambilite snougenn foreiets k hosfirand bolisk as the two four nin\n",
      "memftitul aftings requent in resident engenlishn weam manna than sknega dimence \n",
      "e sit extressipe to the pissition fu the was from words such is syputing the mai\n",
      "t sy lands r boteence of popation us a pradist chiinges dinited and the intinuil\n",
      "cis become uskry as the oflishicors later freric enjade justor and bedias of gen\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.683190 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.674893 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.639509 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2400: 1.656314 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2500: 1.675688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2600: 1.651556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2700: 1.655883 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2800: 1.647479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2900: 1.646052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3000: 1.647170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "x number a lott for lewers in aramailes lombed by was comester for operated in w\n",
      "giting frecesting other addad signifive accentivuter calved andlypers and catklo\n",
      "buts boy was ge trade saum i and a cans and twansing gould syquortical manhoral \n",
      "trese refacced x scusing is henre lipenes the smi vapc bacraas unan poke jagic c\n",
      " players was diseuter madres write last form is soultionown and any nine sever w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3100: 1.624120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3200: 1.641920 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.634719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3400: 1.662254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3500: 1.656772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3600: 1.667196 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3700: 1.642136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.640914 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3900: 1.633078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4000: 1.650266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "lation of the retriared conzerned hame hopus two zero gal cancumanhus nocken cri\n",
      "jersi one eight nine zeath shawtautia ded the nages enformer suist john yeans or\n",
      "version one nine k the icl parliams some the zenowest of octouk of have divasth \n",
      "wish secublican stites pecle strunce dreater he feel status withal quard he gene\n",
      "dised soshifth perier wiftriests of the plar a fourd revodue of the mauke omper \n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4100: 1.630012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.634569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4300: 1.609249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4400: 1.605119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.608090 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4600: 1.610524 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4700: 1.621906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4800: 1.624991 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.627692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5000: 1.605624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "ve is the as a for hypering resons two zero zero zero five cigual wiversplaction\n",
      "will assout scitivess up streee to dero vurdlatively resplitly agifili mijise ch\n",
      "vers worod and deter his jupture bunke of the barr fest say vinitios however acc\n",
      "d and framer amoriggentinal so manuge caustures begin will to are mecedopation r\n",
      "x of lepmel grand offlies computers  wintel unsuach century wadio lewer site the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.601290 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5200: 1.585405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.574789 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.578707 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.565874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5600: 1.573090 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.563830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800: 1.579181 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5900: 1.572101 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.545845 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "z or the reation smist to s shieal in one five zero four fo stace or erepted as \n",
      "holord partories in breit the warpas non one zero zero four is indernetions onat\n",
      "liew dacorate also a corn the armenia shisalens foungen two four minack barijan \n",
      "le policists formel cunoloty eximbunt vettral purport all no with specificical q\n",
      " have doum these may dangebbase the shester as a john only for and he vouse of f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6100: 1.557399 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.530218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.541155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6400: 1.536690 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.553631 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6600: 1.591293 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6700: 1.575536 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6800: 1.600354 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.575081 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 7000: 1.571353 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "zert on the philitation of the note spient swelve alsored islvel regyles provisi\n",
      "reating to be a the not his saguam foreivol alson precks which includar other by\n",
      " cocul right that maso as that have support see harma worls attest fullizive in \n",
      "y the posbally properung or to thought dease a during and nomed a ged an images \n",
      "an amerow is tokva probles oneen s later unadoge prossors in a cual economiclary\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        \n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298617 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.08\n",
      "================================================================================\n",
      "i enzwuxwkxukofged siogbsrfbi lr edeesgmkxet  juhvo g utbosflxbdaglccweserxbotie\n",
      "lgrwl h ensaaaldnztwpi ofldh udzia   xy wyfypicrra tjdsldnazrowluehaufswosvsapmd\n",
      "yqw   gz si ag i k  pii nx nogeml lel aoe  ctqxz xn i meyaabeq egxhmvshmbtinhscp\n",
      "cdpqvgf lie uih eqqeaxj dtmn gnzpr f lynabhhtmekl mstle naubossnvrnltecto rr yos\n",
      "zjtmwtseerdasccoys zcwmgqlx b qifb teealszfnogferl sc snpqid zk bv f  aloxmjnm a\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.582760 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.31\n",
      "Validation set perplexity: 11.90\n",
      "Average loss at step 200: 2.253369 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.30\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 300: 2.084862 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.031146 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.91\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.976804 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.892642 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 700: 1.871556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.868001 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.842532 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 1000: 1.843706 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "================================================================================\n",
      " species morie the has us one nine six three reguent one nine five zero sprusise\n",
      "verion in grileds st yobak hal has ramt to c driban ath mooving and kuta and e l\n",
      "pogoise or teppternance of their overains of the twos earkh mal or ore outs of h\n",
      "k s book it be nuth noma sp coulm voreven lio gored huslired renevor overoning c\n",
      " new sark with f wivece ismst been of stricscan inmon and his stens this riserfa\n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1100: 1.797401 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1200: 1.769292 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1300: 1.760797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1400: 1.762529 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 1500: 1.747109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1600: 1.730953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1700: 1.714144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1800: 1.691890 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1900: 1.693561 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2000: 1.675866 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "zer antheremells and tquecinine if dor seemental extrent from each is and usbany\n",
      "dest may mirromistanimy their corkuesizal deaghated sope trake lests same severv\n",
      "gakity with a now swiden lebble for candanceas auceds file the matestely roun co\n",
      "ural out his orss from suinglating gatters genal as backs five and by rousy he n\n",
      "d inderestant these stargooss of espems have an expinent snashee with they that \n",
      "================================================================================\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 2100: 1.691338 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2200: 1.706330 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 2300: 1.707330 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.681635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2500: 1.686343 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 2600: 1.669568 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2700: 1.681126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.681170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2900: 1.672425 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 3000: 1.679135 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "k b one nine seven b one six six seven o dep ticher becouth claabfot to the many\n",
      " corberlard formation of ephure feltures englishin and grissiace engrali to elom\n",
      "y many one thicagers to we oppolded to in constide mambure and propeventy the of\n",
      "d in certe welk devellenged with strunned with corrmited docal to acror or range\n",
      "xiv no normanoss alloke sity o his the wm mether dfr mulks jecomberists zero lof\n",
      "================================================================================\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3100: 1.647579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3200: 1.636454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3300: 1.643910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3400: 1.630780 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3500: 1.671766 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3600: 1.649166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 3700: 1.648005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 3800: 1.658975 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3900: 1.645973 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4000: 1.639227 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "thonals phelepipate if acclence lines saurnins the and the viden was with offici\n",
      "jegh is rgesse the dains including countries with was form mussio often pajes ha\n",
      "tions actor d formas jase experival ussel cates hown excression of the its cloun\n",
      "ther say hease duroxing beliear lating the suchre the fuvle three chose two s th\n",
      "hs dorpe freewn exument e did xayem the for hyblent foryand yorle in one of the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4100: 1.614656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4200: 1.615736 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4300: 1.616808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4400: 1.604531 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4500: 1.638615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.617560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4700: 1.618174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4800: 1.604359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4900: 1.612129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5000: 1.606506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "neshov wends adam at discibive owneds doesfified to indods foundo can and svess \n",
      "d location which albbute of reamising of a final in the one eight six eight one \n",
      "y of one nine nine two four but restetional xime the eecest volved inlonest of b\n",
      "owmily ejuates of a ald of seven a common born with for why the televection ekbl\n",
      "rently com nation piig lived the propoter defect one areas for bowed to the coxi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.581991 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5200: 1.586847 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5300: 1.589873 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5400: 1.590212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5500: 1.587252 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5600: 1.556364 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5700: 1.574651 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5800: 1.592542 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5900: 1.576668 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6000: 1.579486 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "h myrtholishn tratise aman alstanaria non bager and has thougina no other right \n",
      "onan argard and crosuinal late reads court was were the class rapor from for num\n",
      "tains vide of papan sket two madd city two eight two six fungtion the xamalist i\n",
      "ification of the eupote inlogra introduced in not mitriem in in an indepolitels \n",
      "d were the chine threa lagier one seven sim three eyternal had dooban k ledd the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6100: 1.569768 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6200: 1.579913 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6300: 1.581429 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.570145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6500: 1.553307 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6600: 1.595576 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6700: 1.569949 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6800: 1.573050 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6900: 1.566499 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 7000: 1.586966 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "dof and peasishe rossaine at the of jan wing untwredord tourgly of refe rus amim\n",
      "taes also were sampine for europey road batiakel was be rapuna were two campzoly\n",
      "der who cpedia was ruigons bull the persion meaning u a yoalor re plage e tropon\n",
      "belo shar youise four three compult arterned one de the cave of hanged badge how\n",
      "xy had the plepted to reaths may be in the seven may beenson sustlay anyphy afte\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters  \n",
    "    sx = tf.concat([ix, fx, cx, ox], 1)\n",
    "    sm = tf.concat([im, fm, cm, om], 1)\n",
    "    sb = tf.concat([ib, fb, cb, ob], 1)\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        smatmul = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        smatmul_input, smatmul_forget, update, smatmul_output = tf.split(smatmul, 4, 1)\n",
    "        input_gate = tf.sigmoid(smatmul_input)\n",
    "        forget_gate = tf.sigmoid(smatmul_forget)\n",
    "        output_gate = tf.sigmoid(smatmul_output)\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-109-0803919a7eea>:58: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "WARNING:tensorflow:From /Users/heany/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.301487 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.15\n",
      "================================================================================\n",
      "ku jyelauklyerle tivgl wauowdtyrs zng jkbyns  berre bcmprb  eikgjdntnmes lugn tn\n",
      "kllgr dqy bpgnoo o es reqntqhhirnpftfnkiotc amde  ycper ofovte s s wgm  ayrnnrng\n",
      "uiiutygnnpgh ltvscdmwcz  mknm n fdwytbezwycd oycnl myfke  tt e  w tvtvbegr  tabe\n",
      "r ed dsv yymoh wnz ovgyhtbgdje eweigearsedqtom  s  arixt auwcltrtnhe ikgvalerawl\n",
      "veth yhnx yo lluagip femeclqaz watxoge nexuujeusixheqs od noo  dm yefeot iutare \n",
      "================================================================================\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 100: 2.295653 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.98\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 200: 2.028454 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 300: 1.928632 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 400: 1.877883 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 500: 1.892690 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 600: 1.830644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 700: 1.814933 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 800: 1.800263 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 900: 1.792416 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1000: 1.731347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "her phrosans gigh mapated by of which or soctend of voter conts unix ff is commo\n",
      "ioral attore screents or in one nine from in we one zero eight zero three one si\n",
      "red is one nine three six untim in the enommerson abork one eight ictroppier usi\n",
      "onkodica is neblic of as wis eququerartions is bick orderting integen of some s \n",
      "ximition of explacie poalls stingaldive blactar a clasigued boy the one nine eig\n",
      "================================================================================\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1100: 1.710499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1200: 1.738557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1300: 1.722353 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.701079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1500: 1.694316 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1600: 1.691602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1700: 1.713258 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1800: 1.681876 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1900: 1.685053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2000: 1.697678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "ibts homelisty reated austrogom four heave serming look labsects thise stateximi\n",
      "refial lates rargesessate lacritions to partists sid who phyature a killoke mose\n",
      "flive semptus taded three eather not appolinoasusts the efflence of poble milliz\n",
      "rylople to or other in plance lex the bactrode as the unough peness seldectorigs\n",
      "ybre noture the shanch at raf every norogande stately pakesty founduge all nothe\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.685707 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.661373 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.674181 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2400: 1.672055 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2500: 1.695916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2600: 1.666100 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2700: 1.682253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2800: 1.646108 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2900: 1.653702 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3000: 1.655210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "liti chemoriuse airditatiin kings anteum marious levanially b on austrightses su\n",
      "gition the releigians were stadge his the anasketish vical seericht deathse weiv\n",
      "ustrary chalifer anamins ared such processral it the refoir licklatin posekd of \n",
      "ime east yor choroun of the saighted from apers islible of he wekards of acroso \n",
      "zarion stargge and quigpical of called theirizy of lill a dublan on the engines \n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3100: 1.656223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3200: 1.649194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3300: 1.632334 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3400: 1.636458 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3500: 1.633016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3600: 1.633758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.635548 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3800: 1.624223 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3900: 1.623644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4000: 1.628732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "zame masses for marching these crirea serv to orthoughonerahia latter firm for k\n",
      "lated at scient sent and naturen banly one four servicative battorborned seals b\n",
      "swital throuch of formating by subbazlams and skew his was lafkari halso commond\n",
      "xom sailly tat etictured in thrued in are rsournal ordainta the suctice houghwaz\n",
      "olog clate buttiancol mulweeked desirent atcury chorthinahagally nueing and comp\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4100: 1.627524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 4200: 1.618757 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4300: 1.597445 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4400: 1.629623 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4500: 1.640111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4600: 1.643061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4700: 1.612224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 4800: 1.598709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4900: 1.612228 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5000: 1.637696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "y spligialo seversian a by belar from ossix mlrostrica mand manors not from prai\n",
      "innes in way the curbitensor the first in extinglela the compositio of hor sobu \n",
      "ak that tria pax saina electination adderning arrear mingovile the long sigther \n",
      "ps newspolity islammin nine sole for his has most americation many it screvely o\n",
      "ologian the one while tube economic paying has two zide howa if other crien che \n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5100: 1.624444 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5200: 1.608654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5300: 1.574451 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5400: 1.572664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.563613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5600: 1.591604 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5700: 1.551292 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5800: 1.554857 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5900: 1.574563 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6000: 1.541901 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "arce this quartonovitical in mermance the war tatable be name of object not star\n",
      "gogdeated unt has wavely of intersan brassaryce have tolavely having into zero n\n",
      "ylous with peose was he or light on the then wing conercornon ed one nine nine e\n",
      "g becauble in university bunded they and not that offert which is a elliavard tw\n",
      "king moloc at burised and in effactors color discordiancisly cartbility creignak\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6100: 1.560527 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.582593 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.591205 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6400: 1.620357 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6500: 1.615433 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6600: 1.583307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6700: 1.571092 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 6800: 1.556453 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6900: 1.544762 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 7000: 1.558230 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "als calcidution pequare how have gop outs work of the typical muddpt economy to \n",
      "tent and the urinarian mythex of the down und was ikhe it sost have with nowes a\n",
      "que for the sainarry barburing fist they georginies franks so clusing x two arip\n",
      "queers apols in the leak system for the sactle while att well averieval commenta\n",
      "kerciacious one zearo thrisholy crued by first air pley greentes in britishen or\n",
      "================================================================================\n",
      "Validation set perplexity: 4.49\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(i, dimension=1))\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "        output, state = lstm_cell(i_embed, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.295491 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "rzrhzwet  axfkox etbclyfod aelnaelydudgah ji rgjed ztpktmef  nmae zwblghuh aw eol\n",
      "daiuikuocols eq sg aqftliyf af rxexnu gblavsgrr ilxbiq gfroeluvz nmbp umh y  kuoe\n",
      "pyktctesi pmnnpla e fweosouukuevykr yayhera eh rwbwipfgiqtn y sas  kd  yibw etirw\n",
      "bzlever  ftfvvcexpstcnbod itsxe n emgyy kolialikekep iecosoftxibw w xb phpeqsgmti\n",
      "lbhpptroqi aactsrr reoztfzn t d  kyiej ittwohwiuo uaectnr brbetnecqelsje flzbgeef\n",
      "================================================================================\n",
      "Validation set perplexity: 20.51\n",
      "Average loss at step 100: 2.274939 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 8.24\n",
      "Average loss at step 200: 1.975556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 300: 1.890148 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 400: 1.831871 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 7.26\n",
      "Average loss at step 500: 1.768382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 600: 1.765192 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 700: 1.744996 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 800: 1.728664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 900: 1.720706 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 1000: 1.687979 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "wgband chanzpt he are led reved doufdgen book place of this stea the laigenoap na\n",
      "gpwer touwas an exchematings from how no whhat by tietoms organin the parchounds \n",
      "example ares resine of examplemorzohind as neinst this be show foons a pobon volu\n",
      "xuarle of his verreturended over one fern social detef the base nited on led cuic\n",
      "ved light alfienern and the callean ange bided professed the partgy of formers se\n",
      "================================================================================\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 1100: 1.695114 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 1200: 1.692375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 1300: 1.690191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 1400: 1.664455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1500: 1.656855 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1600: 1.641720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 1700: 1.650501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 1800: 1.668824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 1900: 1.653825 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 6.75\n",
      "Average loss at step 2000: 1.661836 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "qqdate othen mid some mahis a ds westationaly accur ls on cambriched to breeats o\n",
      "djector hels lipsol or by have for metle logian the netwolloots mollion one nine \n",
      "yleary attack and cane may s in the cart ailand in lef imarron freedu a relards a\n",
      "bartly insirits class alls of stuny causer term elaterations to than with only lo\n",
      "tking and sheles documost will even sivery  educat that the unders which a rate m\n",
      "================================================================================\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 2100: 1.647378 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 2200: 1.664990 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 2300: 1.646335 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 2400: 1.642058 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 2500: 1.652878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 2600: 1.646668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 2700: 1.624433 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 2800: 1.625219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 2900: 1.621429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 3000: 1.640999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "================================================================================\n",
      "qavoilos set of more ild serve collegus snecuthoosons by which this richael trek \n",
      "numbro glanued clyis has the ction s idea lock eled as not contines becamed dna i\n",
      "so bists of chronomics which mlike which emulguenging liberry arrich to easternis\n",
      "tch use are were treek tin beco is mayr clystance is are new tricatensif member s\n",
      "ws element emilians the splare in musea race three engurical of mains alveoitia h\n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 3100: 1.608932 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 3200: 1.625127 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 3300: 1.627542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.04\n",
      "Average loss at step 3400: 1.621880 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 3500: 1.607505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3600: 1.630513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 3700: 1.601361 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 3800: 1.602996 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 3900: 1.591136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 4000: 1.608116 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "rsed divish mercenwas sive gapreaturh one nine intromanianie has artun reneeriest\n",
      " then one nine nine six zero occuded other of lang show main swrernor invested at\n",
      "o one nine zero two nines one eight one one one two one nine four one nine the fa\n",
      "fying a south sate belue to bign was law led butics than lear way flaxapp somalar\n",
      "cgate one the edgest perank marral audoe intemself lor and one cer i highels them\n",
      "================================================================================\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 4100: 1.620688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.09\n",
      "Average loss at step 4200: 1.599270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 4300: 1.570035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4400: 1.597524 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.98\n",
      "Average loss at step 4500: 1.584087 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 6.69\n",
      "Average loss at step 4600: 1.588955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 4700: 1.605704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4800: 1.595514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4900: 1.614168 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 5000: 1.627658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "fdael the ernal games the ge and the election definited somear nine times films a\n",
      "o and the whole has processor green trale was results visited in tone three one s\n",
      "qviders one vadvanced two well under that this and purfenerate and the wack nick \n",
      "nite of last and pera who countronyeeal had resoonments in foress and internal te\n",
      "bourg of that this eight three pressing people the spart of fait result during th\n",
      "================================================================================\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 5100: 1.587980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 5200: 1.590523 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5300: 1.569343 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5400: 1.559675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 5500: 1.561011 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 5600: 1.546308 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 5700: 1.580754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 5800: 1.572306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 5900: 1.574388 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 6000: 1.536102 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "================================================================================\n",
      "out vocating two five nine five zero zero tree one nine sped sharestly as an law \n",
      "ybat essentegent denoming and one nine four basia a herb truggs volving doubl bc \n",
      "mkey datc to attem desis names raccedp own feurpous fift tor machinite lain grout\n",
      "ight through the e elarently matcholitiversonal case dows cipps proce when the wi\n",
      "bfame for third  eitson in willes mount has open one six five zero one nine six o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 6100: 1.582087 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6200: 1.584722 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6300: 1.570853 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6400: 1.586663 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 6500: 1.583106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 6600: 1.573106 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 6700: 1.563733 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6800: 1.579342 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 6900: 1.608038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 7000: 1.589141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "smi boinition body for an a an ended to two lejet tolleys is devic a spaicies hom\n",
      "two six el exten petroe the nlerti system orded not to debar these and the intern\n",
      "zpular paraments two six five one nine nine zero one six zero jul to six filem ch\n",
      "g faunder our esticle programzmed an allow in the sames of examplens definition f\n",
      " invege spark are team tladds swas unior the most claid of the m s the f un ii ma\n",
      "================================================================================\n",
      "Validation set perplexity: 6.45\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({\n",
    "                            sample_input[0]: feed[0],\n",
    "                            sample_input[1]: feed[1]\n",
    "                        })\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "                })\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add dropout now to the inputs and outputs only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "num_nodes = 64\n",
    "keep_prob_train = 1.0\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    vocabulary_embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size * vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "  \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_chars = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_chars[:-1], train_chars[1:])\n",
    "    train_labels = train_data[2:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        bigram_index = tf.argmax(i[0], dimension=1) + vocabulary_size * tf.argmax(i[1], dimension=1)\n",
    "        i_embed = tf.nn.embedding_lookup(vocabulary_embeddings, bigram_index)\n",
    "        drop_i = tf.nn.dropout(i_embed, keep_prob_train)\n",
    "        output, state = lstm_cell(drop_i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        drop_logits = tf.nn.dropout(logits, keep_prob_train)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 15000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    #sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    keep_prob_sample = tf.placeholder(tf.float32)\n",
    "    sample_input = list()\n",
    "    for _ in range(2):\n",
    "        sample_input.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "    samp_in_index = tf.argmax(sample_input[0], dimension=1) + vocabulary_size * tf.argmax(sample_input[1], dimension=1)\n",
    "    sample_input_embedding = tf.nn.embedding_lookup(vocabulary_embeddings, samp_in_index)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input_embedding, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.304634 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.24\n",
      "================================================================================\n",
      "wlvnqjqcpitbxvmhceyt pfedn r t qf xndxonuu rzzhkyewjwjeasaqfnbjx hst co  widjpwzm\n",
      "tbcencuitclv ytmvyv iwtib eru elnj s hvwf mmh  eieop  zil hoavhoniivqxte l ye q r\n",
      "veaob cyobmedapxsp rpsiwcxdiox rv dppn fxlamcfa if ss ckiita b t tnqd xzzqs cec u\n",
      "ibhep r ue ee neyzpn ekqlhay ikobahiai jezzzotos  vbfeyvdi hogxguswn zzkrimp utae\n",
      "vgny tyr liihgvta  ozsjk cagagfvri  sriziw kjujn auetwy taat bym brcnsmeaeduktshe\n",
      "================================================================================\n",
      "Validation set perplexity: 19.62\n",
      "Average loss at step 100: 2.295412 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.58\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 200: 1.949385 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 300: 1.861681 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 400: 1.800168 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.763290 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 600: 1.767538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 700: 1.732349 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 800: 1.698969 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 900: 1.718812 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 7.55\n",
      "Average loss at step 1000: 1.733209 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "ginkopcsm ofference and cat hyphonoc in the alpwas of noved to and he conseptits \n",
      "ywo on contras dedimos two while brits chnively mused in fiber moin their have ou\n",
      "khmellin king moven from work in s jaculta is heav world augual and greathin the \n",
      "t since al ping wived by bonial coloman phics its yer he chican an a x da monalit\n",
      "xf have cesses recocistu conorth depart is of lowe undexters see one connection i\n",
      "================================================================================\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 1100: 1.685464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 1200: 1.674341 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 1300: 1.659369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.22\n",
      "Average loss at step 1400: 1.676147 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 1500: 1.663023 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 1600: 1.683826 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 1700: 1.656715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 1800: 1.626604 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 1900: 1.599991 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 2000: 1.645241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "================================================================================\n",
      "ous haracter leveoplock org it and ter hes is ire coaguanguage known ircris propa\n",
      "uwealinks anarchists of to in american rageotarial skies tradesroun columine if t\n",
      "lent found seveludelole normuling the meanicimated society and sseyer weaa deashi\n",
      "in external spate and tbased in lork nine nine one nine john dizdireg souths anx \n",
      "ghere language he of the founce three six drreven newl and statensived which new \n",
      "================================================================================\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 2100: 1.636841 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 2200: 1.630631 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 2300: 1.603562 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 6.82\n",
      "Average loss at step 2400: 1.618107 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 7.00\n",
      "Average loss at step 2500: 1.648771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 2600: 1.621156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 2700: 1.637871 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 2800: 1.622388 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 2900: 1.622072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 3000: 1.628728 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "vns monthalberean called billynated in changlish thereston lanquired hols most in\n",
      "uq indisistorish germany and a four first or factionals parstance for with were w\n",
      "ore instave being francisms tade religion b his calso resulted thomb john to long\n",
      "hapters songer or defenlery any hord of prissae iters one eight six one nine six \n",
      "dxt greaten unline executs of romation wedge he passon son was liberforchs proces\n",
      "================================================================================\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 3100: 1.608924 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 3200: 1.630878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 3300: 1.621432 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 3400: 1.648987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 3500: 1.646313 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 3600: 1.660522 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 3700: 1.636980 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 3800: 1.636035 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.20\n",
      "Average loss at step 3900: 1.628846 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 4000: 1.643893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "================================================================================\n",
      "gpermaca briticas mancest s crime of sea broidays were diffences emperope its six\n",
      "ae city is nafted it declimanly it a with two th primar day the three kaka wherma\n",
      "bxrored the peze restrical cict flowing the to from councill to the resurregifica\n",
      "a schology s the val recemperially or s been oldeents has her true but of armosay\n",
      "w keb one six a from the gore eletter a players that exties for at the foots dire\n",
      "================================================================================\n",
      "Validation set perplexity: 7.08\n",
      "Average loss at step 4100: 1.625397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 4200: 1.628500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 4300: 1.610454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 4400: 1.603434 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.72\n",
      "Average loss at step 4500: 1.610857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.11\n",
      "Average loss at step 4600: 1.611106 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 4700: 1.623144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 4800: 1.631936 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4900: 1.626928 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 5000: 1.605869 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "xxontive cosm by one six six five six zero would murdecries these that originomic\n",
      "ogical for year havely workis many pertaining for his seen ancied somen frone nin\n",
      "xq ruth aszone two five four zero zero zero unsvrent often may two zero zero zero\n",
      "dwidn of commonold one nine six ken one  ism triograms iderature to singer solve \n",
      "zing the rich is to from wwho grandire one nine nine nine nine mininite first cil\n",
      "================================================================================\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 5100: 1.617940 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 5200: 1.596113 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 5300: 1.586892 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 5400: 1.593624 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 5500: 1.578270 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 5600: 1.588725 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 5700: 1.581918 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 5800: 1.593348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 5900: 1.588341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 6000: 1.568591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "imf the city fighter agnither yal bits directions and despeake number and on one \n",
      "aaace steuce that the ah pition for aquare she amongs are raw also websed stocmen\n",
      "hxls may classibary collegistince is may that the souns a number untival part ant\n",
      "wd started elecond of the structions ens of the unitizens three extealeter in etc\n",
      "pde human aare are ensur talbori is and bitaria its smics our serie takes irone t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 6100: 1.587625 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 6200: 1.554425 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 6.86\n",
      "Average loss at step 6300: 1.561375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 6400: 1.555475 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 6500: 1.573788 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 6.77\n",
      "Average loss at step 6600: 1.612476 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 6700: 1.595818 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 6800: 1.619380 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 6900: 1.589376 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 7.02\n",
      "Average loss at step 7000: 1.586579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "kking that down the form action beatr comming a layer of the was byns spinsure fo\n",
      "ger by those in tran purch not in a version for rihh wall healt product the was a\n",
      "gshind of jean henized queen in one nine and in veches to ability furity a capsid\n",
      "zfaching wies meaning one britz hire in one nine zero air run tertunt aracteacing\n",
      "jgling of the on the daul active alway and piup be is the gipeven not is regional\n",
      "================================================================================\n",
      "Validation set perplexity: 6.98\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = collections.deque(maxlen=2)\n",
    "                    for _ in range(2):  \n",
    "                        feed.append(random_distribution())\n",
    "                    sentence = characters(feed[0])[0] + characters(feed[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({\n",
    "                            sample_input[0]: feed[0],\n",
    "                            sample_input[1]: feed[1]\n",
    "                        })\n",
    "                        feed.append(sample(prediction))\n",
    "                        sentence += characters(feed[1])[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({\n",
    "                    sample_input[0]: b[0],\n",
    "                    sample_input[1]: b[1]\n",
    "                })\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "**(difficult!)**\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "\n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "\n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](https://arxiv.org/abs/1409.3215) for best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
